SUBJECT: &NAME Hi &NAME , a quick question : I have been looking at perplexity values for the LMs I 've generated , and I find that on the whole the test set perplexities for the bigrams are significantly higher than for the unigrams . This is undoubtedly due to the fact that whenever a bigram is unseen , I backoff to the unigram distribution , but since the probabilities are calculated over the discounted probability mass , they are much lower than in the standard unigram case . However , the bigrams still perform significantly better , because even though the backed-off unigram probabilities are much smaller , they are still correctly proportioned . Thus , it seems that comparing perplexities for different ngrams is basically pointless - what do you think ? &CHAR 